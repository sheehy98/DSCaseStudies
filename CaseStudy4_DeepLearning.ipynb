{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 4 : Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEAM Members:** Please EDIT this cell and add the names of all the team members in your team\n",
    "\n",
    "    Mago Sheehy\n",
    "    \n",
    "    Brian Phillips\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Load in the movie review data, create TF-IDF features, and use two of your favorite classification algorithms from sci-kit learn for predicting sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This problem is, basically, already answered as part of case study 3, so it is fine to use your work from there to help answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing useful libraries and classes\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import timeit\n",
    "import numpy as np\n",
    "import twitter\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from urllib.parse import unquote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training data folder must be passed as first argument\n",
    "dataset = load_files('txt_sentoken', shuffle=False)\n",
    "\n",
    "# Split the dataset in training and test set:\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
    "\n",
    "# Turning the testing and training docs into TF-IDF tokens \n",
    "vectorized = TfidfVectorizer(ngram_range = (1, 1)).fit(docs_train)\n",
    "Xtrain = vectorized.transform(docs_train)\n",
    "Xtest = vectorized.transform(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.89      0.84      0.87       254\n",
      "         pos       0.85      0.89      0.87       246\n",
      "\n",
      "    accuracy                           0.87       500\n",
      "   macro avg       0.87      0.87      0.87       500\n",
      "weighted avg       0.87      0.87      0.87       500\n",
      "\n",
      "KNeighborsClassifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.76      0.32      0.45       254\n",
      "         pos       0.56      0.89      0.69       246\n",
      "\n",
      "    accuracy                           0.60       500\n",
      "   macro avg       0.66      0.61      0.57       500\n",
      "weighted avg       0.66      0.60      0.57       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fitting a LinearSVC model to the training data and comparing the model to the testing data\n",
    "y_predicted = LinearSVC(penalty = 'l2', C = 10).fit(Xtrain, y_train).predict(Xtest)\n",
    "\n",
    "# Printing a report on the model's accuracy\n",
    "print(\"LinearSVC:\\n\", metrics.classification_report(y_test, y_predicted, target_names=dataset.target_names))\n",
    "\n",
    "\n",
    "# Fitting a KNeighborsClassifier model to the training data and comparing the model to the testing data\n",
    "y_predicted = KNeighborsClassifier(n_neighbors = 3).fit(Xtrain, y_train).predict(Xtest)\n",
    "\n",
    "# Printing a report on the model's accuracy\n",
    "print(\"KNeighborsClassifier:\\n\", metrics.classification_report(y_test, y_predicted, target_names=dataset.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Use a Multi-Layer Perceptron (MLP) for classifying the reviews.  Explore the parameters for the MLP and compare the accuracies against your baseline algorithms in Problem 1.\n",
    "\n",
    "**Read the documentation for the MLPClassifier class at https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier.** \n",
    "* Try different values for \"hidden_layer_sizes\".  What do you observe in terms of accuracy?\n",
    "* Try different values for \"activation\". What do you observe in terms of accuracy?\n",
    "* Try different values for \"solver\". What do you observe in terms of accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing MLPClassifier with different hidden_layer_sizes\n",
      "\n",
      "Performing MLPClassifier with different activation\n",
      "\n",
      "Performing MLPClassifier with different solver\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Creating the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('clf', MLPClassifier())\n",
    "])\n",
    "\n",
    "# Creating a set of parameters and running MLPClassifier on the testing and training\n",
    "# data with each of them. Each parameter is adjusted individually to avoid a huge\n",
    "# number of computations\n",
    "parameters = {\n",
    "    'clf__hidden_layer_sizes': [(200,), (100,), (100, 2), (50, 2), (10, 5), (5, 20)],\n",
    "}\n",
    "print('Performing MLPClassifier with different hidden_layer_sizes\\n')\n",
    "grid_searchH = GridSearchCV(pipeline, parameters, cv = 5, n_jobs=-1)\n",
    "grid_searchH.fit(Xtrain, y_train)\n",
    "n_candidatesH = len(grid_searchH.cv_results_['params'])\n",
    "\n",
    "parameters = {\n",
    "    'clf__activation' : ['identity', 'tanh', 'logistic', 'relu'],\n",
    "}\n",
    "print('Performing MLPClassifier with different activation\\n')\n",
    "grid_searchA = GridSearchCV(pipeline, parameters, cv = 5, n_jobs=-1)\n",
    "grid_searchA.fit(Xtrain, y_train)\n",
    "n_candidatesA = len(grid_searchA.cv_results_['params'])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'clf__solver' : ['lbfgs', 'sgd', 'adam'],\n",
    "}\n",
    "print('Performing MLPClassifier with different solver\\n')\n",
    "grid_searchS = GridSearchCV(pipeline, parameters, cv = 5, n_jobs=-1)\n",
    "grid_searchS.fit(Xtrain, y_train)\n",
    "n_candidatesS = len(grid_searchS.cv_results_['params'])\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MLPClassifier with {'clf__hidden_layer_sizes': (200,)} \n",
      "\n",
      "0.8246666666666667\n",
      "\n",
      "MLPClassifier with {'clf__hidden_layer_sizes': (100,)} \n",
      "\n",
      "0.8266666666666668\n",
      "\n",
      "MLPClassifier with {'clf__hidden_layer_sizes': (100, 2)} \n",
      "\n",
      "0.6433333333333333\n",
      "\n",
      "MLPClassifier with {'clf__hidden_layer_sizes': (50, 2)} \n",
      "\n",
      "0.5553333333333335\n",
      "\n",
      "MLPClassifier with {'clf__hidden_layer_sizes': (10, 5)} \n",
      "\n",
      "0.82\n",
      "\n",
      "MLPClassifier with {'clf__hidden_layer_sizes': (5, 20)} \n",
      "\n",
      "0.8273333333333334\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_candidatesH):\n",
    "    print(\"\\nMLPClassifier with\", grid_searchH.cv_results_['params'][i], \"\\n\")\n",
    "    print(grid_searchH.cv_results_['mean_test_score'][i])\n",
    "    \n",
    "# Adding one hidden layer with 200 nodes and adding one hidden layer with 100 nodes had \n",
    "# very similar accuracies (200 nodes had the slight edge), adding two layers of size 100\n",
    "# produced a significantly less accurate machine than the machine with one layer of the \n",
    "# same size, two layers of 50 were less accurate than two layers of 100, and increasing \n",
    "# the number of layers beyond two(even when decreaseing the number of nodes) increased \n",
    "# accuracy.  We avoided implementing machines with high node counts and layer counts to\n",
    "# be nice to our computers and to avoid overfitting.  When testing our machines on new \n",
    "# data we may want to try using both the one layer of 200 and the one layer of 100 machines\n",
    "# to ensure that the increased accuracy of the machine with 200 nodes is not due to \n",
    "# overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MLPClassifier with {'clf__activation': 'identity'} \n",
      "\n",
      "0.8273333333333334\n",
      "\n",
      "MLPClassifier with {'clf__activation': 'tanh'} \n",
      "\n",
      "0.8313333333333333\n",
      "\n",
      "MLPClassifier with {'clf__activation': 'logistic'} \n",
      "\n",
      "0.8346666666666666\n",
      "\n",
      "MLPClassifier with {'clf__activation': 'relu'} \n",
      "\n",
      "0.828\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_candidatesA):\n",
    "    print(\"\\nMLPClassifier with\", grid_searchA.cv_results_['params'][i], \"\\n\")\n",
    "    print(grid_searchA.cv_results_['mean_test_score'][i])\n",
    "    \n",
    "# Using the 'identity', 'tanh', 'logistic', and 'relu' activation functions all\n",
    "# produce very similar accuracies (when using the default 100 nodes and one layer).\n",
    "# This is notable because it means that the problem can relatively accurately be \n",
    "# represented as a linear problem (as the identity function makes good predictions).\n",
    "# The logistic function preformed the best and the relu function performed the worst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MLPClassifier with {'clf__solver': 'lbfgs'} \n",
      "\n",
      "0.8400000000000001\n",
      "\n",
      "MLPClassifier with {'clf__solver': 'sgd'} \n",
      "\n",
      "0.49800000000000005\n",
      "\n",
      "MLPClassifier with {'clf__solver': 'adam'} \n",
      "\n",
      "0.826\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_candidatesS):\n",
    "    print(\"\\nMLPClassifier with\", grid_searchS.cv_results_['params'][i], \"\\n\")\n",
    "    print(grid_searchS.cv_results_['mean_test_score'][i])\n",
    "    \n",
    "# The 'lbfgs' and 'adam' solvers had similar accuracies, while the 'sdg' solver\n",
    "# was not able to run when the rest of the parameters were set to their default\n",
    "# values.  'adam' performed the best and 'lbfgs' performed the worst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing MLPClassifier with every combination of top 2 parameters for hidden_layer_sizes, activation, and solver:\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Running the MLPClassifier with every combination of top 2 parameters for\n",
    "# hidden_layer_sizes, activation, and solver.\n",
    "parameters = {\n",
    "    'clf__hidden_layer_sizes': [(200,), (100,)],\n",
    "    'clf__activation' : ['identity', 'logistic'],\n",
    "    'clf__solver' : ['lbfgs', 'adam'],\n",
    "}\n",
    "print(\"Performing MLPClassifier with every combination of top 2 parameters for hidden_layer_sizes, activation, and solver:\\n\")\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv = 5, n_jobs=1)\n",
    "grid_search.fit(Xtrain, y_train)\n",
    "n_candidates = len(grid_search.cv_results_['params'])\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MLPClassifier with {'clf__activation': 'identity', 'clf__hidden_layer_sizes': (200,), 'clf__solver': 'lbfgs'} \n",
      "\n",
      "0.8373333333333333\n",
      "\n",
      "MLPClassifier with {'clf__activation': 'identity', 'clf__hidden_layer_sizes': (200,), 'clf__solver': 'adam'} \n",
      "\n",
      "0.8313333333333335\n",
      "\n",
      "MLPClassifier with {'clf__activation': 'identity', 'clf__hidden_layer_sizes': (100,), 'clf__solver': 'lbfgs'} \n",
      "\n",
      "0.836\n",
      "\n",
      "MLPClassifier with {'clf__activation': 'identity', 'clf__hidden_layer_sizes': (100,), 'clf__solver': 'adam'} \n",
      "\n",
      "0.8286666666666666\n",
      "\n",
      "MLPClassifier with {'clf__activation': 'logistic', 'clf__hidden_layer_sizes': (200,), 'clf__solver': 'lbfgs'} \n",
      "\n",
      "0.72\n",
      "\n",
      "MLPClassifier with {'clf__activation': 'logistic', 'clf__hidden_layer_sizes': (200,), 'clf__solver': 'adam'} \n",
      "\n",
      "0.8333333333333334\n",
      "\n",
      "MLPClassifier with {'clf__activation': 'logistic', 'clf__hidden_layer_sizes': (100,), 'clf__solver': 'lbfgs'} \n",
      "\n",
      "0.836\n",
      "\n",
      "MLPClassifier with {'clf__activation': 'logistic', 'clf__hidden_layer_sizes': (100,), 'clf__solver': 'adam'} \n",
      "\n",
      "0.834\n",
      "\n",
      " Best parameters for MLPClassifier:  {'clf__activation': 'identity', 'clf__hidden_layer_sizes': (200,), 'clf__solver': 'lbfgs'}\n",
      "\n",
      " Score with best parameters:  0.8373333333333333\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_candidates):\n",
    "    print(\"\\nMLPClassifier with\", grid_search.cv_results_['params'][i], \"\\n\")\n",
    "    print(grid_search.cv_results_['mean_test_score'][i])\n",
    "\n",
    "print(\"\\n Best parameters for MLPClassifier: \", grid_search.best_params_)\n",
    "print(\"\\n Score with best parameters: \", grid_search.best_score_)\n",
    "\n",
    "# The set of parameters that produced the machine with the highest accuracy:\n",
    "# {'clf__activation': 'logistic', 'clf__hidden_layer_sizes': (100,), 'clf__solver': 'adam'}\n",
    "# The set of parameters that produced the machine with the lowest accuracy:\n",
    "# {'clf__activation': 'logistic', 'clf__hidden_layer_sizes': (200,), 'clf__solver': 'lbfgs'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Accuracy is not everything!  How fast are the algorithms versus their accuracy?\n",
    "**Compare the runtime of your  baseline algorithms to the runtime of the MLPClassifier** \n",
    "\n",
    "**The jupyter command %timeit can be used to measure how long a calculation takes https://ipython.readthedocs.io/en/stable/interactive/magics.html.**\n",
    "* Try different values for \"hidden_layer_sizes\".  What do you observe in term of runtime?\n",
    "* Try different values for \"activation\". What do you observe in term of runtime?\n",
    "* Try different values for \"solver\". What do you observe in term of runtime?\n",
    "* How long does the \"fit\" function take as opposed to the \"predict\" function?  Can you explain why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup = \"\"\"import sys;\n",
    "from sklearn.feature_extraction.text import CountVectorizer;\n",
    "from sklearn.feature_extraction.text import TfidfTransformer;\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer;\n",
    "from sklearn.svm import LinearSVC;\n",
    "from sklearn.pipeline import Pipeline;\n",
    "from sklearn.model_selection import GridSearchCV;\n",
    "from sklearn.datasets import load_files;\n",
    "from sklearn.model_selection import train_test_split;\n",
    "from sklearn import metrics;\n",
    "from sklearn.neighbors import KNeighborsClassifier;\n",
    "from sklearn.neural_network import MLPClassifier;\n",
    "\n",
    "dataset = load_files('txt_sentoken', shuffle=False);\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.25, random_state=None);\n",
    "vectorized = TfidfVectorizer(ngram_range = (1, 1)).fit(docs_train);\n",
    "Xtrain = vectorized.transform(docs_train);\n",
    "Xtest = vectorized.transform(docs_test);\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier time for hidden_layer_sizes: (100, '') \n",
      " 65.498 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier time for hidden_layer_sizes: (50, 2) \n",
      " 98.028 seconds\n",
      "\n",
      "MLPClassifier time for hidden_layer_sizes: (10, 2) \n",
      " 10.98 seconds\n",
      "\n",
      "MLPClassifier time for hidden_layer_sizes: (10, 5) \n",
      " 10.841 seconds\n",
      "\n",
      "MLPClassifier time for hidden_layer_sizes: (5, 5) \n",
      " 7.156 seconds\n",
      "\n",
      "MLPClassifier time for activation: identity \n",
      " 66.035 seconds\n",
      "\n",
      "MLPClassifier time for activation: tanh \n",
      " 67.079 seconds\n",
      "\n",
      "MLPClassifier time for activation: logistic \n",
      " 145.971 seconds\n",
      "\n",
      "MLPClassifier time for activation: relu \n",
      " 66.032 seconds\n",
      "\n",
      "MLPClassifier time for solver: lbfgs \n",
      " 23.373 seconds\n",
      "\n",
      "MLPClassifier time for solver: adam \n",
      " 65.669 seconds\n",
      "\n",
      "MLPClassifier default parameters fit time:\n",
      " 67.38 seconds\n",
      "\n",
      "MLPClassifier default parameters predict time:\n",
      " 0.017 seconds\n",
      "\n",
      "LinearSVC time:\n",
      " 0.272 seconds\n",
      "\n",
      "KNeighborsClassifiers time:\n",
      " 0.203 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = [(100, ''), (50, 2), (10, 2), (10, 5), (5, 5)]\n",
    "for i in parameters :\n",
    "    stmt = 'MLPClassifier(hidden_layer_sizes = (' + \", \".join(map(str,i)) + ')).fit(Xtrain, y_train).predict(Xtest)'\n",
    "    print('MLPClassifier time for hidden_layer_sizes:', i, '\\n', round(timeit.timeit(setup = setup, stmt = stmt, number = 1), 3), \"seconds\\n\")\n",
    "\n",
    "    \n",
    "parameters = ['identity', 'tanh', 'logistic', 'relu']\n",
    "for i in parameters :\n",
    "    stmt = 'MLPClassifier(activation = \\'' + i +'\\').fit(Xtrain, y_train).predict(Xtest)'\n",
    "    print('MLPClassifier time for activation:', i, \"\\n\", round(timeit.timeit(setup = setup, stmt = stmt, number = 1), 3), \"seconds\\n\")\n",
    "\n",
    "    \n",
    "parameters = ['lbfgs', 'adam']\n",
    "for i in parameters :\n",
    "    stmt = 'MLPClassifier(solver = \\'' + i +'\\').fit(Xtrain, y_train).predict(Xtest)'\n",
    "    print('MLPClassifier time for solver:', i, \"\\n\", round(timeit.timeit(setup = setup, stmt = stmt, number = 1), 3), \"seconds\\n\")\n",
    "\n",
    "    \n",
    "stmt = 'MLPClassifier().fit(Xtrain, y_train)'\n",
    "print('MLPClassifier default parameters fit time:\\n', round(timeit.timeit(setup = setup, stmt = stmt, number = 1), 3), \"seconds\\n\")\n",
    "\n",
    "\n",
    "setup2 = setup + '\\ny = MLPClassifier().fit(Xtrain, y_train)'\n",
    "stmt = 'y.predict(Xtest)'\n",
    "print('MLPClassifier default parameters predict time:\\n', round(timeit.timeit(setup = setup2, stmt = stmt, number = 1), 3), \"seconds\\n\")\n",
    "\n",
    "\n",
    "stmt = 'LinearSVC(penalty = \\'l2\\', C = 10).fit(Xtrain, y_train).predict(Xtest)'\n",
    "print('LinearSVC time:\\n', round(timeit.timeit(setup = setup, stmt = stmt, number = 1), 3), \"seconds\\n\")\n",
    "\n",
    "stmt = 'KNeighborsClassifier(n_neighbors = 3).fit(Xtrain, y_train).predict(Xtest)'\n",
    "print('KNeighborsClassifiers time:\\n', round(timeit.timeit(setup = setup, stmt = stmt, number = 1), 3), \"seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Problem 4: Business question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Suppose you had a machine learning algorithms that could detect the sentinment of tweets that was highly accurate.  What kind of business could you build around that?\n",
    "* Who would be your competitors, and what are their sizes?\n",
    "* What would be the size of the market for your product?\n",
    "* In addition, assume that your machine learning was slow to train, but fast in making predicitions on new data.  How would that affect your business plan?\n",
    "* How could you use the cloud to support your product?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSUMER_KEY = \"RB4hX8gjnUlPX4Ijvuj5gL9LT\"\n",
    "CONSUMER_SECRET = \"YovCvfis70dTuD1IuZMdHdhfiPPAr5nd22QkTIpnELq4r7Dw9j\"\n",
    "OAUTH_TOKEN = \"571213367-fyYadzmC7wGWOkM6OCF99ZevVjWGDC3fnO5OoYGr\"\n",
    "OAUTH_TOKEN_SECRET = \"OjRD5By0qU0q3g9DJXCpMvnJrdYe1KIj2G2BoGtRng9q5\"\n",
    "\n",
    "auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                           CONSUMER_KEY, CONSUMER_SECRET)\n",
    "\n",
    "twitter_api = twitter.Twitter(auth=auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets about Netflix are 45.477 % positve\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "\n",
    "q = 'Netflix'\n",
    "count = 100\n",
    "search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "statuses = search_results['statuses']\n",
    "\n",
    "# Iterating through more batches of results by following the cursor\n",
    "while len(tweets) < 400:\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    except KeyError as e: # No more results when next_results doesn't exist\n",
    "        break\n",
    "\n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=847960489447628799&q=%23RIPSelena&count=100&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
    "\n",
    "    search_results = twitter_api.search.tweets(**kwargs)\n",
    "    statuses = search_results['statuses']\n",
    "    for i in statuses:\n",
    "        if (not ('retweeted_status' in i.keys())) and i['lang'] == 'en':\n",
    "            tweets.append(str(i['text']))\n",
    "    \n",
    "tweets2 = []               \n",
    "for i in tweets:\n",
    "    tweets2.append(bytes(i, 'utf-8'))\n",
    "X = TfidfVectorizer(ngram_range = (1, 1)).fit(docs_train)\n",
    "Xtest = X.transform(tweets2)\n",
    "y_predictor = MLPClassifier(activation = 'logistic', hidden_layer_sizes = (100,), solver = 'adam').fit(Xtrain, y_train)\n",
    "y_predicted = y_predictor.predict(Xtest)\n",
    "print(\"Tweets about Netflix are\", round(np.count_nonzero(y_predicted == 1)/len(tweets)*100, 3), \"% positve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets about Hulu are 30.865 % positve\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "\n",
    "q = 'Hulu'\n",
    "count = 100\n",
    "search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "statuses = search_results['statuses']\n",
    "\n",
    "# Iterating through more batches of results by following the cursor\n",
    "while len(tweets) < 400:\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    except KeyError as e: # No more results when next_results doesn't exist\n",
    "        break\n",
    "\n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=847960489447628799&q=%23RIPSelena&count=100&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
    "\n",
    "    search_results = twitter_api.search.tweets(**kwargs)\n",
    "    statuses = search_results['statuses']\n",
    "    for i in statuses:\n",
    "        if (not ('retweeted_status' in i.keys())) and i['lang'] == 'en':\n",
    "            tweets.append(str(i['text']))\n",
    "    \n",
    "tweets2 = []               \n",
    "for i in tweets:\n",
    "    tweets2.append(bytes(i, 'utf-8'))\n",
    "Xtest = X.transform(tweets2)\n",
    "y_predicted = y_predictor.predict(Xtest)\n",
    "print(\"Tweets about Hulu are\", round(np.count_nonzero(y_predicted == 1)/len(tweets)*100, 3), \"% positve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets about Amazon Prime Video are 40.05 % positve\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "\n",
    "q = 'Prime Video'\n",
    "count = 100\n",
    "search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "statuses = search_results['statuses']\n",
    "\n",
    "# Iterating through more batches of results by following the cursor\n",
    "while len(tweets) < 400:\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    except KeyError as e: # No more results when next_results doesn't exist\n",
    "        break\n",
    "\n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=847960489447628799&q=%23RIPSelena&count=100&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
    "\n",
    "    search_results = twitter_api.search.tweets(**kwargs)\n",
    "    statuses = search_results['statuses']\n",
    "    for i in statuses:\n",
    "        if (not ('retweeted_status' in i.keys())) and i['lang'] == 'en':\n",
    "            tweets.append(str(i['text']))\n",
    "    \n",
    "tweets2 = []               \n",
    "for i in tweets:\n",
    "    tweets2.append(bytes(i, 'utf-8'))\n",
    "Xtest = X.transform(tweets2)\n",
    "y_predicted = y_predictor.predict(Xtest)\n",
    "print(\"Tweets about Amazon Prime Video are\", round(np.count_nonzero(y_predicted == 1)/len(tweets)*100, 3), \"% positve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets about Disney+ are 65.783 % positve\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "\n",
    "q = 'Disney+'\n",
    "count = 100\n",
    "search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "statuses = search_results['statuses']\n",
    "\n",
    "# Iterating through more batches of results by following the cursor\n",
    "while len(tweets) < 400:\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    except KeyError as e: # No more results when next_results doesn't exist\n",
    "        break\n",
    "\n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=847960489447628799&q=%23RIPSelena&count=100&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
    "\n",
    "    search_results = twitter_api.search.tweets(**kwargs)\n",
    "    statuses = search_results['statuses']\n",
    "    for i in statuses:\n",
    "        if (not ('retweeted_status' in i.keys())) and i['lang'] == 'en':\n",
    "            tweets.append(str(i['text']))\n",
    "    \n",
    "tweets2 = []               \n",
    "for i in tweets:\n",
    "    tweets2.append(bytes(i, 'utf-8'))\n",
    "Xtest = X.transform(tweets2)\n",
    "y_predicted = y_predictor.predict(Xtest)\n",
    "print(\"Tweets about Disney+ are\", round(np.count_nonzero(y_predicted == 1)/len(tweets)*100, 3), \"% positve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets about 'horrible' are 20.948 % positve\n"
     ]
    }
   ],
   "source": [
    "# Evidence that the MLPClassifier is correctly handling twitter data\n",
    "tweets = []\n",
    "\n",
    "q = 'horrible'\n",
    "count = 100\n",
    "search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "statuses = search_results['statuses']\n",
    "\n",
    "# Iterating through more batches of results by following the cursor\n",
    "while len(tweets) < 400:\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    except KeyError as e: # No more results when next_results doesn't exist\n",
    "        break\n",
    "\n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=847960489447628799&q=%23RIPSelena&count=100&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
    "\n",
    "    search_results = twitter_api.search.tweets(**kwargs)\n",
    "    statuses = search_results['statuses']\n",
    "    for i in statuses:\n",
    "        if (not ('retweeted_status' in i.keys())) and i['lang'] == 'en':\n",
    "            tweets.append(str(i['text']))\n",
    "    \n",
    "tweets2 = []               \n",
    "for i in tweets:\n",
    "    tweets2.append(bytes(i, 'utf-8'))\n",
    "Xtest = X.transform(tweets2)\n",
    "y_predicted = y_predictor.predict(Xtest)\n",
    "print(\"Tweets about 'horrible' are\", round(np.count_nonzero(y_predicted == 1)/len(tweets)*100, 3), \"% positve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets about 'perfect' are 71.462 % positve\n"
     ]
    }
   ],
   "source": [
    "# Evidence that the MLPClassifier is correctly handling twitter data\n",
    "tweets = []\n",
    "\n",
    "q = 'perfect'\n",
    "count = 100\n",
    "search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "statuses = search_results['statuses']\n",
    "\n",
    "# Iterating through more batches of results by following the cursor\n",
    "while len(tweets) < 400:\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    except KeyError as e: # No more results when next_results doesn't exist\n",
    "        break\n",
    "\n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=847960489447628799&q=%23RIPSelena&count=100&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
    "\n",
    "    search_results = twitter_api.search.tweets(**kwargs)\n",
    "    statuses = search_results['statuses']\n",
    "    for i in statuses:\n",
    "        if (not ('retweeted_status' in i.keys())) and i['lang'] == 'en':\n",
    "            tweets.append(str(i['text']))\n",
    "    \n",
    "tweets2 = []               \n",
    "for i in tweets:\n",
    "    tweets2.append(bytes(i, 'utf-8'))\n",
    "Xtest = X.transform(tweets2)\n",
    "y_predicted = y_predictor.predict(Xtest)\n",
    "print(\"Tweets about 'perfect' are\", round(np.count_nonzero(y_predicted == 1)/len(tweets)*100, 3), \"% positve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets about Toy Story are 65.281 % positve\n"
     ]
    }
   ],
   "source": [
    "#Use high tomato ratings to search different movies and their genres\n",
    "#Toy story and 2 (Animation,Adventure,Comedy,Family,Fantasy)\n",
    "#Mary Poppins (Comedy,Family,Fantasy,Musical)\n",
    "#Pinoochio (Adventure,Comedy,Drama,Family,Musical,Romance)\n",
    "#The Many Adventures of Winnie the Pooh (Animation,Adventure,Comedy,Family,Musical)\n",
    "#Tinker Bell (Animation,Adventure,Family,Fantasy)\n",
    "tweets = []\n",
    "\n",
    "q = 'Toy Story'\n",
    "count = 100\n",
    "search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "statuses = search_results['statuses']\n",
    "\n",
    "# Iterating through more batches of results by following the cursor\n",
    "while len(tweets) < 400:\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    except KeyError as e: # No more results when next_results doesn't exist\n",
    "        break\n",
    "\n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=847960489447628799&q=%23RIPSelena&count=100&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
    "\n",
    "    search_results = twitter_api.search.tweets(**kwargs)\n",
    "    statuses = search_results['statuses']\n",
    "    for i in statuses:\n",
    "        if (not ('retweeted_status' in i.keys())) and i['lang'] == 'en':\n",
    "            tweets.append(str(i['text']))\n",
    "    \n",
    "tweets2 = []               \n",
    "for i in tweets:\n",
    "    tweets2.append(bytes(i, 'utf-8'))\n",
    "X = TfidfVectorizer(ngram_range = (1, 1)).fit(docs_train)\n",
    "Xtest = X.transform(tweets2)\n",
    "y_predicted = y_predictor.predict(Xtest)\n",
    "print(\"Tweets about Toy Story are\", round(np.count_nonzero(y_predicted == 1)/len(tweets)*100, 3), \"% positve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets about Mary Poppins are 45.721 % positve\n"
     ]
    }
   ],
   "source": [
    "#Use high tomato ratings to search different movies and their genres\n",
    "#Toy story and 2 (Animation,Adventure,Comedy,Family,Fantasy)\n",
    "#Mary Poppins (Comedy,Family,Fantasy,Musical)\n",
    "#Pinoochio (Adventure,Comedy,Drama,Family,Musical,Romance)\n",
    "#The Many Adventures of Winnie the Pooh (Animation,Adventure,Comedy,Family,Musical)\n",
    "#Tinker Bell (Animation,Adventure,Family,Fantasy)\n",
    "tweets = []\n",
    "\n",
    "q = 'Mary Poppins'\n",
    "count = 100\n",
    "search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "statuses = search_results['statuses']\n",
    "\n",
    "# Iterating through more batches of results by following the cursor\n",
    "while len(tweets) < 400:\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    except KeyError as e: # No more results when next_results doesn't exist\n",
    "        break\n",
    "\n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=847960489447628799&q=%23RIPSelena&count=100&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
    "\n",
    "    search_results = twitter_api.search.tweets(**kwargs)\n",
    "    statuses = search_results['statuses']\n",
    "    for i in statuses:\n",
    "        if (not ('retweeted_status' in i.keys())) and i['lang'] == 'en':\n",
    "            tweets.append(str(i['text']))\n",
    "    \n",
    "tweets2 = []               \n",
    "for i in tweets:\n",
    "    tweets2.append(bytes(i, 'utf-8'))\n",
    "X = TfidfVectorizer(ngram_range = (1, 1)).fit(docs_train)\n",
    "Xtest = X.transform(tweets2)\n",
    "y_predicted = y_predictor.predict(Xtest)\n",
    "print(\"Tweets about Mary Poppins are\", round(np.count_nonzero(y_predicted == 1)/len(tweets)*100, 3), \"% positve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets about Old Yeller are 59.081 % positve\n"
     ]
    }
   ],
   "source": [
    "#Use high tomato ratings to search different movies and their genres\n",
    "#Toy story and 2 (Animation,Adventure,Comedy,Family,Fantasy)\n",
    "#Mary Poppins (Comedy,Family,Fantasy,Musical)\n",
    "#Pinoochio (Adventure,Comedy,Drama,Family,Musical,Romance)\n",
    "#The Many Adventures of Winnie the Pooh (Animation,Adventure,Comedy,Family,Musical)\n",
    "#Tinker Bell (Animation,Adventure,Family,Fantasy)\n",
    "tweets = []\n",
    "\n",
    "q = 'Old Yeller'\n",
    "count = 100\n",
    "search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "statuses = search_results['statuses']\n",
    "\n",
    "# Iterating through more batches of results by following the cursor\n",
    "while len(tweets) < 400:\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    except KeyError as e: # No more results when next_results doesn't exist\n",
    "        break\n",
    "\n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=847960489447628799&q=%23RIPSelena&count=100&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
    "\n",
    "    search_results = twitter_api.search.tweets(**kwargs)\n",
    "    statuses = search_results['statuses']\n",
    "    for i in statuses:\n",
    "        if (not ('retweeted_status' in i.keys())) and i['lang'] == 'en':\n",
    "            tweets.append(str(i['text']))\n",
    "    \n",
    "tweets2 = []               \n",
    "for i in tweets:\n",
    "    tweets2.append(bytes(i, 'utf-8'))\n",
    "X = TfidfVectorizer(ngram_range = (1, 1)).fit(docs_train)\n",
    "Xtest = X.transform(tweets2)\n",
    "y_predicted = y_predictor.predict(Xtest)\n",
    "print(\"Tweets about Old Yeller are\", round(np.count_nonzero(y_predicted == 1)/len(tweets)*100, 3), \"% positve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets about Winnie the Pooh are 52.164 % positve\n"
     ]
    }
   ],
   "source": [
    "#Use high tomato ratings to search different movies and their genres\n",
    "#Toy story and 2 (Animation,Adventure,Comedy,Family,Fantasy)\n",
    "#Mary Poppins (Comedy,Family,Fantasy,Musical)\n",
    "#Pinoochio (Adventure,Comedy,Drama,Family,Musical,Romance)\n",
    "#The Many Adventures of Winnie the Pooh (Animation,Adventure,Comedy,Family,Musical)\n",
    "#Tinker Bell (Animation,Adventure,Family,Fantasy)\n",
    "tweets = []\n",
    "\n",
    "q = 'Winnie the Pooh'\n",
    "count = 100\n",
    "search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "statuses = search_results['statuses']\n",
    "\n",
    "# Iterating through more batches of results by following the cursor\n",
    "while len(tweets) < 400:\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    except KeyError as e: # No more results when next_results doesn't exist\n",
    "        break\n",
    "\n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=847960489447628799&q=%23RIPSelena&count=100&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
    "\n",
    "    search_results = twitter_api.search.tweets(**kwargs)\n",
    "    statuses = search_results['statuses']\n",
    "    for i in statuses:\n",
    "        if (not ('retweeted_status' in i.keys())) and i['lang'] == 'en':\n",
    "            tweets.append(str(i['text']))\n",
    "    \n",
    "tweets2 = []               \n",
    "for i in tweets:\n",
    "    tweets2.append(bytes(i, 'utf-8'))\n",
    "X = TfidfVectorizer(ngram_range = (1, 1)).fit(docs_train)\n",
    "Xtest = X.transform(tweets2)\n",
    "y_predicted = y_predictor.predict(Xtest)\n",
    "print(\"Tweets about Winnie the Pooh are\", round(np.count_nonzero(y_predicted == 1)/len(tweets)*100, 3), \"% positve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets about Tinker Bell are 32.353 % positve\n"
     ]
    }
   ],
   "source": [
    "#Use high tomato ratings to search different movies and their genres\n",
    "#Toy story and 2 (Animation,Adventure,Comedy,Family,Fantasy)\n",
    "#Mary Poppins (Comedy,Family,Fantasy,Musical)\n",
    "#Pinoochio (Adventure,Comedy,Drama,Family,Musical,Romance)\n",
    "#The Many Adventures of Winnie the Pooh (Animation,Adventure,Comedy,Family,Musical)\n",
    "#Tinker Bell (Animation,Adventure,Family,Fantasy)\n",
    "tweets = []\n",
    "\n",
    "q = 'Tinker Bell'\n",
    "count = 100\n",
    "search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "statuses = search_results['statuses']\n",
    "\n",
    "# Iterating through more batches of results by following the cursor\n",
    "while len(tweets) < 400:\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    except KeyError as e: # No more results when next_results doesn't exist\n",
    "        break\n",
    "\n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=847960489447628799&q=%23RIPSelena&count=100&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
    "\n",
    "    search_results = twitter_api.search.tweets(**kwargs)\n",
    "    statuses = search_results['statuses']\n",
    "    for i in statuses:\n",
    "        if (not ('retweeted_status' in i.keys())) and i['lang'] == 'en':\n",
    "            tweets.append(str(i['text']))\n",
    "    \n",
    "tweets2 = []               \n",
    "for i in tweets:\n",
    "    tweets2.append(bytes(i, 'utf-8'))\n",
    "X = TfidfVectorizer(ngram_range = (1, 1)).fit(docs_train)\n",
    "Xtest = X.transform(tweets2)\n",
    "y_predicted = y_predictor.predict(Xtest)\n",
    "print(\"Tweets about Tinker Bell are\", round(np.count_nonzero(y_predicted == 1)/len(tweets)*100, 3), \"% positve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "0.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
