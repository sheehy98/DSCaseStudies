{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 3 : Textual analysis of movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://getthematic.com/wp-content/uploads/2018/03/Harris-Word-Cloud-e1522406279125.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEAM Members:** Please EDIT this cell and add the names of all the team members in your team\n",
    "\n",
    "    Mago Sheehy\n",
    "    \n",
    "    Brian Phillips\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Sentiment Analysis on movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Installing scikit-learn using Anaconda does not necessarily download the example source-code.\n",
    "* Accordingly, you may need to download these directly from Gitub at https://github.com/scikit-learn/scikit-learn:\n",
    "    * The data can be downloaded using doc/tutorial/text_analytics/data/movie_reviews/fetch_data.py\n",
    "    * A skeleton for the solution can be found in doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py\n",
    "    * A completed solution can be found in doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py\n",
    "* **It is ok to use the solution provided in the scikit-learn distribution as a starting place for your work.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import numpy as np\n",
    "import twitter\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from urllib.parse import unquote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 2000\n"
     ]
    }
   ],
   "source": [
    "# the training data folder must be passed as first argument\n",
    "dataset = load_files('txt_sentoken', shuffle=False)\n",
    "print(\"n_samples: %d\" % len(dataset.data))\n",
    "\n",
    "# split the dataset in training and test set:\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.25, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Build a vectorizer / classifier pipeline that filters out tokens\n",
    "# that are too rare or too frequent\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(min_df = 3, max_df = 0.95)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LinearSVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        CountVectorizer(max_df=0.95, min_df=3)),\n",
       "                                       ('tfidf', TfidfTransformer()),\n",
       "                                       ('clf', LinearSVC())]),\n",
       "             n_jobs=-1, param_grid={'vect__ngram_range': [(1, 1), (2, 2)]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TASK: Build a grid search to find out whether unigrams or bigrams are\n",
    "# more useful.\n",
    "# Fit the pipeline on the training set using grid search for the parameters\n",
    "\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (2, 2)],\n",
    "}\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv = 5, n_jobs=-1)\n",
    "grid_search.fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 word: mean - 0.84; std - 0.02\n",
      "2 word: mean - 0.81; std - 0.02\n"
     ]
    }
   ],
   "source": [
    "# TASK: print the cross-validated scores for the each parameters set\n",
    "# explored by the grid search\n",
    "\n",
    "n_candidates = len(grid_search.cv_results_['params'])\n",
    "for i in range(n_candidates):\n",
    "    print(i + 1, 'word: mean - %0.2f; std - %0.2f' % (grid_search.cv_results_['mean_test_score'][i], grid_search.cv_results_['std_test_score'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Predict the outcome on the testing set and store it in a variable\n",
    "# named y_predicted\n",
    "\n",
    "y_predicted = grid_search.predict(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.87      0.82      0.84       237\n",
      "         pos       0.85      0.89      0.87       263\n",
      "\n",
      "    accuracy                           0.86       500\n",
      "   macro avg       0.86      0.85      0.86       500\n",
      "weighted avg       0.86      0.86      0.86       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the classification report\n",
    "print(metrics.classification_report(y_test, y_predicted, target_names=dataset.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[195  42]\n",
      " [ 30 233]]\n"
     ]
    }
   ],
   "source": [
    "# Print and plot the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Explore the scikit-learn TfidVectorizer class\n",
    "\n",
    "**Read the documentation for the TfidVectorizer class at http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html.** \n",
    "* Define the term frequencyâ€“inverse document frequency (TF-IDF) statistic (http://en.wikipedia.org/wiki/Tf%E2%80%93idf will likely help).\n",
    "* Run the TfidVectorizer class on the training data above (docs_train).\n",
    "* Explore the min_df and max_df parameters of TfidVectorizer.  What do they mean? How do they change the features you get?\n",
    "* Explore the ngram_range parameter of TfidVectorizer.  What does it mean? How does it change the features you get? (Note, large values  of ngram_range may take a long time to run!)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "The term frequency-inverse document frequency statistic measures how important each word is.  It does this by recording the frequency of each word (rather than counting the number of times the word appears to avoid words that exist in longer articles appearing more important), while accounting for unimportant words by decreasing the values of words that exist in many documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 35302)\n"
     ]
    }
   ],
   "source": [
    "vectorized = TfidfVectorizer(ngram_range = (1, 1)).fit_transform(docs_train)\n",
    "print(vectorized.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "min_df is a parameter that determines how infrequently a term must be used before it is ingnored.  If the input is an integer, words that occur less than the specified number of times in total are refused entry to the structure, and if the input is a float, words that occur in a smaller proportion of the documents than the specified number are refused entry.\n",
    "\n",
    "max_df is a parameter that determines how frequently a term must be used before it is ingnored.  If the input is an integer, words that occur more than the specified number of times in total are refused entry to the structure, and if the input is a float, words that occur in a greater proportion of the documents than the specified number are refused entry.\n",
    "\n",
    "Using min_df can ensure typos don't find their way into the set of features, and can account for unimportant proper nouns or technical words that are not important or prevalent enough to have a bearing on future predictions.\n",
    "\n",
    "Using max_df can ensure that common words that may have not been properly handled by the 'inverse document frequency' aspect of tf-idf are not used to make predictions.\n",
    "\n",
    "Using these parameters decrease the overall number of features (their defaults are to place no bounds on frequency or infrequency), but it can also improve the accuracy of tf-idf as a whole."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ngram_range is a parameter that determines the number of words that each feature might consist of.  'ngram_range' takes two parameters, a minimum and a maximum.  The minimum represents the fewest number of words that a feature can have, and the maximum represents the largest.  If '1' is included in the range, for example, every word will be a feature, while if '2' is included, every set of adjacent words will also correspond to a feature.  When larger numbers are included in the range, phrases can be used to make predictions, which can make these predictions more accurate, but increasing the range significantly increases the time and storage required.  The default values for this range are 1 and 1, meaning only single words correspond to features of their own. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Machine learning algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Based upon Problem 2 pick some parameters for TfidfVectorizer\n",
    "    * \"fit\" your TfidfVectorizer using docs_train\n",
    "    * Compute \"Xtrain\", a Tf-idf-weighted document-term matrix using the transform function on docs_train\n",
    "    * Compute \"Xtest\", a Tf-idf-weighted document-term matrix using the transform function on docs_test\n",
    "    * Note, be sure to use the same Tf-idf-weighted class (**\"fit\" using docs_train**) to transform **both** docs_test and docs_train\n",
    "* Examine two classifiers provided by scikit-learn \n",
    "    * LinearSVC\n",
    "    * KNeighborsClassifier\n",
    "    * Try a number of different parameter settings for each and judge your performance using a confusion matrix (see Problem 1 for an example).\n",
    "* Does one classifier, or one set of parameters work better?\n",
    "    * Why do you think it might be working better?\n",
    "* For a particular choice of parameters and classifier, look at 2 examples where the prediction was incorrect.\n",
    "    * Can you conjecture on why the classifier made a mistake for this prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = TfidfVectorizer(ngram_range = (1, 1)).fit(docs_train)\n",
    "Xtrain = vectorized.transform(docs_train)\n",
    "Xtest = vectorized.transform(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LinearSVC with {'clf__C': 1, 'clf__fit_intercept': True} \n",
      " [[206  38]\n",
      " [ 33 223]]\n",
      "0.8253333333333334\n",
      "\n",
      "LinearSVC with {'clf__C': 1, 'clf__fit_intercept': False} \n",
      " [[207  37]\n",
      " [ 30 226]]\n",
      "0.828\n",
      "\n",
      "LinearSVC with {'clf__C': 5, 'clf__fit_intercept': True} \n",
      " [[206  38]\n",
      " [ 30 226]]\n",
      "0.8166666666666667\n",
      "\n",
      "LinearSVC with {'clf__C': 5, 'clf__fit_intercept': False} \n",
      " [[206  38]\n",
      " [ 30 226]]\n",
      "0.8246666666666667\n",
      "\n",
      "LinearSVC with {'clf__C': 10, 'clf__fit_intercept': True} \n",
      " [[206  38]\n",
      " [ 31 225]]\n",
      "0.818\n",
      "\n",
      "LinearSVC with {'clf__C': 10, 'clf__fit_intercept': False} \n",
      " [[207  37]\n",
      " [ 30 226]]\n",
      "0.82\n",
      "\n",
      "LinearSVC with {'clf__C': 50, 'clf__fit_intercept': True} \n",
      " [[206  38]\n",
      " [ 32 224]]\n",
      "0.8200000000000001\n",
      "\n",
      "LinearSVC with {'clf__C': 50, 'clf__fit_intercept': False} \n",
      " [[207  37]\n",
      " [ 31 225]]\n",
      "0.82\n",
      "\n",
      "LinearSVC with {'clf__C': 100, 'clf__fit_intercept': True} \n",
      " [[206  38]\n",
      " [ 32 224]]\n",
      "0.8200000000000001\n",
      "\n",
      "LinearSVC with {'clf__C': 100, 'clf__fit_intercept': False} \n",
      " [[207  37]\n",
      " [ 31 225]]\n",
      "0.82\n",
      "\n",
      "LinearSVC with {'clf__C': 500, 'clf__fit_intercept': True} \n",
      " [[206  38]\n",
      " [ 31 225]]\n",
      "0.8213333333333332\n",
      "\n",
      "LinearSVC with {'clf__C': 500, 'clf__fit_intercept': False} \n",
      " [[207  37]\n",
      " [ 31 225]]\n",
      "0.8206666666666667\n",
      "\n",
      " Best parameters for LinearSVC:  {'clf__C': 1, 'clf__fit_intercept': False}\n",
      "\n",
      " Mean with best parameters:  0.828\n"
     ]
    }
   ],
   "source": [
    "#linear = LinearSVC().fit(Xtrain, y_train)\n",
    "#y_predicted = linear.predict(Xtest)\n",
    "#print(\"LinearSVC with default parameters:\\n\", metrics.confusion_matrix(y_test, y_predicted))\n",
    "\n",
    "#linear = LinearSVC(C = 1000).fit(Xtrain, y_train)\n",
    "#y_predicted = linear.predict(Xtest)\n",
    "#print(\"\\nLinearSVC with C = 1000:\\n\", metrics.confusion_matrix(y_test, y_predicted))\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'clf__C': [1, 5, 10, 50, 100, 500],\n",
    "    'clf__fit_intercept' : [True, False],\n",
    "}\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv = 5, n_jobs=-1)\n",
    "grid_search.fit(Xtrain, y_train)\n",
    "n_candidates = len(grid_search.cv_results_['params'])\n",
    "\n",
    "for i in range(n_candidates):\n",
    "    y_predicted = LinearSVC(dual = False, fit_intercept = grid_search.cv_results_['param_clf__fit_intercept'][i], C = grid_search.cv_results_['param_clf__C'][i]).fit(Xtrain, y_train).predict(Xtest)\n",
    "    print(\"\\nLinearSVC with\", grid_search.cv_results_['params'][i], \"\\n\", metrics.confusion_matrix(y_test, y_predicted))\n",
    "    print(grid_search.cv_results_['mean_test_score'][i])\n",
    "\n",
    "print(\"\\n Best parameters for LinearSVC: \", grid_search.best_params_)\n",
    "print(\"\\n Mean with best parameters: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNeighborsClassifier with {'clf__n_neighbors': 1, 'clf__weights': 'uniform'} \n",
      " [[106 138]\n",
      " [ 43 213]]\n",
      "0.6406666666666666\n",
      "\n",
      "KNeighborsClassifier with {'clf__n_neighbors': 1, 'clf__weights': 'distance'} \n",
      " [[106 138]\n",
      " [ 43 213]]\n",
      "0.6406666666666666\n",
      "\n",
      "KNeighborsClassifier with {'clf__n_neighbors': 2, 'clf__weights': 'uniform'} \n",
      " [[143 101]\n",
      " [ 71 185]]\n",
      "0.6346666666666667\n",
      "\n",
      "KNeighborsClassifier with {'clf__n_neighbors': 2, 'clf__weights': 'distance'} \n",
      " [[106 138]\n",
      " [ 43 213]]\n",
      "0.6406666666666666\n",
      "\n",
      "KNeighborsClassifier with {'clf__n_neighbors': 3, 'clf__weights': 'uniform'} \n",
      " [[ 79 165]\n",
      " [ 17 239]]\n",
      "0.6013333333333334\n",
      "\n",
      "KNeighborsClassifier with {'clf__n_neighbors': 3, 'clf__weights': 'distance'} \n",
      " [[ 81 163]\n",
      " [ 17 239]]\n",
      "0.6026666666666667\n",
      "\n",
      "KNeighborsClassifier with {'clf__n_neighbors': 5, 'clf__weights': 'uniform'} \n",
      " [[ 57 187]\n",
      " [ 17 239]]\n",
      "0.5559999999999999\n",
      "\n",
      "KNeighborsClassifier with {'clf__n_neighbors': 5, 'clf__weights': 'distance'} \n",
      " [[ 59 185]\n",
      " [ 17 239]]\n",
      "0.5573333333333333\n",
      "\n",
      "KNeighborsClassifier with {'clf__n_neighbors': 10, 'clf__weights': 'uniform'} \n",
      " [[ 31 213]\n",
      " [  8 248]]\n",
      "0.5486666666666666\n",
      "\n",
      "KNeighborsClassifier with {'clf__n_neighbors': 10, 'clf__weights': 'distance'} \n",
      " [[ 30 214]\n",
      " [  6 250]]\n",
      "0.5459999999999999\n",
      "\n",
      "KNeighborsClassifier with {'clf__n_neighbors': 50, 'clf__weights': 'uniform'} \n",
      " [[ 56 188]\n",
      " [  5 251]]\n",
      "0.6166666666666666\n",
      "\n",
      "KNeighborsClassifier with {'clf__n_neighbors': 50, 'clf__weights': 'distance'} \n",
      " [[ 45 199]\n",
      " [  3 253]]\n",
      "0.592\n",
      "\n",
      " Best parameters for KNeighborsClassifier:  {'clf__n_neighbors': 1, 'clf__weights': 'uniform'}\n",
      "\n",
      " Score with best parameters:  0.6406666666666666\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('clf', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'clf__n_neighbors': [1, 2, 3, 5, 10, 50],\n",
    "    'clf__weights' : ['uniform', 'distance'],\n",
    "}\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv = 5, n_jobs=-1)\n",
    "grid_search.fit(Xtrain, y_train)\n",
    "n_candidates = len(grid_search.cv_results_['params'])\n",
    "\n",
    "for i in range(n_candidates):\n",
    "    y_predicted = KNeighborsClassifier(n_neighbors = grid_search.cv_results_['param_clf__n_neighbors'][i], weights = grid_search.cv_results_['param_clf__weights'][i]).fit(Xtrain, y_train).predict(Xtest)\n",
    "    print(\"\\nKNeighborsClassifier with\", grid_search.cv_results_['params'][i], \"\\n\", metrics.confusion_matrix(y_test, y_predicted))\n",
    "    print(grid_search.cv_results_['mean_test_score'][i])\n",
    "\n",
    "print(\"\\n Best parameters for KNeighborsClassifier: \", grid_search.best_params_)\n",
    "print(\"\\n Score with best parameters: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "LinearSVC works better than KNeighborsClassifier, and LinearSVC works best with the parameters C = 5 and fit_intercept = True.  LinearSVC is likely the better classifier because the data is very inexact, and LinearSVC handles this better.  In a model where every category is split into distinct groups, LinearSVC and KNeighborsClassifier should produce very similar results, but the algorithms handle messy data, such as text, in different ways.  The line between a positive and a negative review is very inexact, so with KNeighborsClassifier, every review that exists anywhere near that line is assigned a prediction essentially at random. LinearSVC forces an exact line (in the case of multi-dimensional data, a hyperplane) to be drawn between the categories and classifies inputs by checking them against that line, and while this method still is not great for predicting reviews that are very close to the line, the presence of a line means that it predicts the sentiment of reviews that are a resonable distance from that line with much better (although still far from perfect) accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive:\n",
      "b'it is an understood passion and an understood calm . \\nbud white walks into the home of lynn bracken , a prostitute \" cut to look like veronica lake . \" \\nhe\\'s one of l . a . \\'s finest investigating the murder of fellow cop , and one of the leads takes him to her home . \\nit\\'s understood that he is quiet thunder , a guy who\\'s calm voice is more powerful than his arms . \\nit\\'s understood that she\\'s supposed to be beautiful , but underneath her face is pain and scraped out lines that say her life could have been so much more . \\nyou know without having to be told . \\n \" you\\'re the first guy who hasn\\'t told me i look like veronicca lake in under a minute , \" she says . \\n \" you look ten times better . \" \\nhe says it without thinking . \\nlike he knows without her having to say anything . \\nwhite\\'s face doesn\\'t turn , it doesn\\'t blush . \\nyou see his eyes , and you believe him . \\nit\\'s a perfect moment in a near perfect movie . \\nl . a . confidential is the best movie of the year . \\nit is grace and poetry ; a richly layered work of pure entertainment . \\nit is a portrait of police and morality as it works it\\'s way through characters that are so real on screen that you believe that they might have a soul . \\nin a phrase , it is everything . \\nit is funny , it is exciting , has enough action and adventure and mystery and grace and suspense . \\nit is the pinnacle of a movie , a complete work , a flawless film . \\nit\\'s hard to tell you about the plot , because it\\'s about so much more than plot . \\nbesides , there\\'s too much to describe . \\nsure , it\\'s about police and searching for corruption and who fights for the right reasons and the definition of justice . \\nbut it\\'s also about what you feel when you look into the eyes of the characters in the film . \\ndirector curtis hanson has perfectly framed each character , letting you see into them , rooting for them one moment , doubting them the next . \\nthe actors work with their lines , but in this movie their most important weapon is their eyes . \\nyou can tell what each person is feeling just by looking into their eyes for one moment . \\nrussell crowe plays bud white , a muscle cop with a beef for wife beaters . \\nhis movements are fierce . \\nwhen he strikes , you jump , when he\\'s calm , you know it won\\'t be for long . \\nhis intensity oozes out of the screen . \\nyou follow his character through the movie afraid of him , but in sympathy of him , because you know how good his heart is . \\nhe\\'s looking for a way to solve crimes , you\\'re just never sure how he\\'s going to do it . \\nhis counter is ed exley , played with a chisselled face by guy pierce . \\nyou feel for him , for his character , because of all the cops in the movie he\\'s the one that\\'s actually trying to do the right thing by going by the book . \\nyou know that he must face the reality of breaking the book , and you love the way he gets there . \\ni\\'ve seen l . a confidential twice , immediately after it was over , i wanted to hit the rewind button and watch it again . \\ni didn\\'t want the theater experience to end . \\nit\\'s been a long time since i actually experienced a movie , feeling like i got something after i walked out . \\nthat\\'s why i love movies ; the great ones can be too rich and powerful to describe . \\nwatch l . a . confidential : after you leave the theater , you feel like you just watched a classic movie . \\nthat\\'s why la confidential stands as the best movie of the year . \\nl . a . confidential ( directed by curtis hanson . \\nstarring russell crowe , guy pierce , kevin spacey , james cromwell , and kim basinger . \\n'\n",
      "\n",
      "False negative\n",
      "b'synopsis : al simmons , top-notch assasin with a guilty conscience , dies in a fiery explosion and goes to hell . \\nmaking a pact with malebolgia , a chief demon there , simmons returns to earth 5 years later reborn as spawn , a general in hell\\'s army donning a necroplasmic costume replete with knives , chains , and a morphing cape . \\nsullen , wise cogliostro and flatulating , wisecracking violator vy for spawn\\'s attention . \\ncomments : when todd mcfarlane left marvel comics ( where he had made a name for himself as a first-rate comic book penciller on the \" spider-man \" titles ) to join the newly-formed , creator-owned image comics , a new comic book legend was born : spawn . \\nmcfarlane\\'s \" spawn \" immediately became a commercial and critical success and a defining comic book series of the 1990s . \\nmcfarlane created a hero who was not only original but visually intricate , allowing mcfarlane to utilize his knack for artistic detail to the max . \\nthe early \" spawn \" issues brilliantly capture mcfarlane\\'s genius at illustration and show his early attempts at writing . \\nwith the popularity of \" spawn \" and the success of the current warner bros . \\'s batman film franchise , a movie version of some sort seemed inevitable for spawn . \\nin the summer of 1997 , hence , new line cinema released spawn , a live-action film based on the groundbreaking series . \\nthis topheavy exercise in violence and special effects unfortunately topples quickly and leaves fans of the comic book , like me , numbed by how much spawn misses the mark . \\nwhat happened ? \\nwhy is spawn so bad ? \\ntodd mcfarlane himself executive produced this disappointing misfire and even appears in a cameo . \\ni don\\'t think , however , that his presence necessarily hurt ( or helped ) the film . \\ni place the blame , in part , on the recent hollywood trend , fueled by public demand apparently , for special effects blow-out movies utilizing the latest computer technology . \\nthese films focus upon the effects at the expense of everything else : character , plot , dialogue , etc . spawn , reflecting this trend , shows the audience one gratuitous scene after another populated with morphing characters and filled with unnecessary pyrotechnics . \\nhardly a minute goes by in this film without fires , explosions , knives and chains appearing out of nowhere , glowing eyes , or constantly transforming demons . \\na lot of it is visually interesting and technically solid , don\\'t get me wrong , but , because the script and cast aren\\'t engaging , spawn ultimately comes across like overwrought wallpaper ( the surface may capture the eye , but nothing exists underneath ) . \\nspawn\\'s translation of the comic book suffers the most at the storyline level . \\nmcfarlane\\'s spawn was a tortured hero . \\na mercenary by trade , al simmons was nonetheless a warm man in love with the beautiful wanda . \\nhaving died and journeyed to hell , he made a pact to return to earth to be with wanda . \\nsimmons , however , discovers that his memories are fragmented , his body a creepy mess , and his wife married . \\ndespite his sometimes violent nature , readers couldn\\'t help but feel sympathetic toward his plight as the spawn of the underworld . \\nspawn attempts to show all of this but does not spend nearly the time it should to do so . \\nwhen the characters are developed , they seem absurd rather than touching . \\nthe cartoonish dialogue and implausible subplot ( a general possesses the antidote to a supervirus called heat-16 which he wishes to unleash to enslave the world ) do not help matters . \\nspawn , in an apparent attempt to duplicate the success of batman , also unwisely spends too much time on a villain , the violator ( batman favored the joker over batman ) . \\njohn leguizamo , like jack nicholson in batman , receives top billing in the cast as the violator ; michael jai white ( al simmons / spawn ) is second . \\ni ordinarily find leguizamo an intensely annoying presence in films which seems to make him a perfect candidate for the violator . \\nthe film , however , spends so much time on the violator\\'s offensive antics that they grate on the nerves . \\napparently meant to be the comic relief in the film ( as nicholson was in batman ) , especially when contrasted with the sullen spawn , the violator\\'s lines are oftentimes grotesque and unfunny , leaving the audience wishing he would leave . \\nleguizamo does a satisfactory job in the role , but he is seen far too often in the film . \\nmichael jai white , a relative newcomer to theatrical releases , seems to be an appealing actor , and he handles his role adequately , but we see little of him without various masks on . \\nmore time needed to be spent on white\\'s character before he became spawn for the movie to pull at the heartstrings . \\na special note should be made about martin sheen as the over-the-top , obnoxious , evil general wynn . \\neasily the hammiest performance in the movie , it\\'s hard to imagine how sheen mucked up his role so much ; after all , he played a vietnam assasin brilliantly in the great apocalypse now . \\nsheen\\'s excessive demeanor do not help the audience accept him as a mastermind villain and comes as a surprise considering his extensive career in film . \\nmany other elements conspire with the disappointing script and abundant special effects to drag spawn down . \\nmtv-style , jerky , in-your-face editing is one of them . \\nflames , for example , roll across the screen sometimes to announce a shift in setting . \\ncogliostro , unlikely wannabe guide for spawn , serves as a poor narrator for the film . \\nhe goofily tells the audience , at one point , that \" how much of [spawn\\'s] humanity is left remains to be seen , \" as if the audience really cares as one violent sequence leads to another . \\nthe music , finally , assaults the audience as much as the manic violence and offensive dialogue . \\nloud and obnoxious hard rock fused with drum loops dominate some scenes . \\nto be fair , however , marilyn manson\\'s \" long hard road out of hell \" effectively compliments spawn\\'s return to earth , while filter and the crystal method\\'s \" ( can\\'t you ) trip like i do \" proves a surprisingly fitting theme song . \\nfor as good a comic book as it is , \" spawn \" did not spawn a good movie . \\nspawn , instead , suffers from too much pomp and circumstance , and too little plot and character development . \\nit receives two stars for its technically well-done special effects . \\nmany other films , though , have equal , if not superior , special effects and are much better . \\nrated pg-13 , spawn seems more violent than many r-rated movies and probably wouldn\\'t be appropriate for the very young . \\n'\n"
     ]
    }
   ],
   "source": [
    "# Code to print the predicted and actual values of the first 100 reviews to help find 2 that were wrong \n",
    "# documents 1 and 5 were selected because they were the first false positive and false negative respectively\n",
    "\n",
    "#y_predicted = LinearSVC(penalty = 'l2', C = 10).fit(Xtrain, y_train).predict(Xtest)\n",
    "#for i in range(100):\n",
    "#    print(i, \"Predicted: \", y_predicted[i], \"Actual: \", y_test[i])\n",
    "\n",
    "print(\"False positive:\")\n",
    "print(docs_test[1])\n",
    "# I believe that the classifier falsely assumed this review was positive because it contained many words that \n",
    "# one would expect a positive review to have and only a small portion of the review is actually critical.\n",
    "# Some of the positive words it contains include rewarding, grounded, emotional, interesting, elevate, best, \n",
    "# compelling, and artistic\n",
    "\n",
    "print(\"\\nFalse negative\")\n",
    "print(docs_test[5])\n",
    "# I believe the classifier misidentified this review because it contains a significant ammount of negative\n",
    "# language and there is a section at the end where the reviewer lists off some of the movies flaws.\n",
    "# Some of the negative words it contains include long, dull, trimmer, unfortunately, cliched, regret, and bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 4: Business question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How could NLP be used to generate a Data Science oriented product?\n",
    "\n",
    "* For example, can you take the machine learning algorithm above, which was trained on movie review data, and test it on your Twitter data?\n",
    "\n",
    "* Does this provide a way to tell whether Tweets about a product are either positive or negative!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSUMER_KEY = \"RB4hX8gjnUlPX4Ijvuj5gL9LT\"\n",
    "CONSUMER_SECRET = \"YovCvfis70dTuD1IuZMdHdhfiPPAr5nd22QkTIpnELq4r7Dw9j\"\n",
    "OAUTH_TOKEN = \"571213367-fyYadzmC7wGWOkM6OCF99ZevVjWGDC3fnO5OoYGr\"\n",
    "OAUTH_TOKEN_SECRET = \"OjRD5By0qU0q3g9DJXCpMvnJrdYe1KIj2G2BoGtRng9q5\"\n",
    "\n",
    "auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                           CONSUMER_KEY, CONSUMER_SECRET)\n",
    "\n",
    "twitter_api = twitter.Twitter(auth=auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets about Samsung Galaxy S are 15.8 % positve\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "\n",
    "q = 'Samsung Galaxy S'\n",
    "count = 100\n",
    "search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "statuses = search_results['statuses']\n",
    "\n",
    "# Iterating through more batches of results by following the cursor\n",
    "while len(tweets) < 1000:\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    except KeyError as e: # No more results when next_results doesn't exist\n",
    "        break\n",
    "\n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=847960489447628799&q=%23RIPSelena&count=100&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
    "\n",
    "    search_results = twitter_api.search.tweets(**kwargs)\n",
    "    statuses = search_results['statuses']\n",
    "    for i in statuses:\n",
    "        if (not ('retweeted_status' in i.keys())) and i['lang'] == 'en':\n",
    "            tweets.append(str(i['text']))\n",
    "    \n",
    "tweets2 = []               \n",
    "for i in tweets:\n",
    "    tweets2.append(bytes(i, 'utf-8'))\n",
    "X = TfidfVectorizer(ngram_range = (1, 1)).fit(docs_train)\n",
    "Xtest = X.transform(tweets2)\n",
    "y_predictor = LinearSVC(penalty = 'l2', C = 10).fit(Xtrain, y_train).fit(Xtrain, y_train)\n",
    "y_predicted = y_predictor.predict(Xtest)\n",
    "print(\"Tweets about Samsung Galaxy S are\", round(np.count_nonzero(y_predicted == 1)/len(tweets)*100, 3), \"% positve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets about Samsung Fridge are 14.826 % positve\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "\n",
    "q = 'Samsung Fridge'\n",
    "count = 100\n",
    "search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "statuses = search_results['statuses']\n",
    "\n",
    "# Iterating through more batches of results by following the cursor\n",
    "while len(tweets) < 1000:\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    except KeyError as e: # No more results when next_results doesn't exist\n",
    "        break\n",
    "\n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=847960489447628799&q=%23RIPSelena&count=100&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
    "\n",
    "    search_results = twitter_api.search.tweets(**kwargs)\n",
    "    statuses = search_results['statuses']\n",
    "    for i in statuses:\n",
    "        if (not ('retweeted_status' in i.keys())) and i['lang'] == 'en':\n",
    "            tweets.append(str(i['text']))\n",
    "    \n",
    "tweets2 = []               \n",
    "for i in tweets:\n",
    "    tweets2.append(bytes(i, 'utf-8'))\n",
    "Xtest = X.transform(tweets2)\n",
    "y_predicted = y_predictor.predict(Xtest)\n",
    "print(\"Tweets about Samsung Fridge are\", round(np.count_nonzero(y_predicted == 1)/len(tweets)*100, 3), \"% positve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets about the Samsung Buds Pro are  38.31775700934579 % positve\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "\n",
    "q = 'Samsung Buds Pro'\n",
    "count = 100\n",
    "search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "statuses = search_results['statuses']\n",
    "\n",
    "# Iterating through more batches of results by following the cursor\n",
    "while len(tweets) < 1000:\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    except KeyError as e: # No more results when next_results doesn't exist\n",
    "        break\n",
    "\n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=847960489447628799&q=%23RIPSelena&count=100&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
    "\n",
    "    search_results = twitter_api.search.tweets(**kwargs)\n",
    "    statuses = search_results['statuses']\n",
    "    for i in statuses:\n",
    "        if (not ('retweeted_status' in i.keys())) and i['lang'] == 'en':\n",
    "            tweets.append(str(i['text']))\n",
    "    \n",
    "tweets2 = []               \n",
    "for i in tweets:\n",
    "    tweets2.append(bytes(i, 'utf-8'))\n",
    "Xtest = X.transform(tweets2)\n",
    "y_predicted = y_predictor.predict(Xtest)\n",
    "print(\"Tweets about the Samsung Buds Pro are \", np.count_nonzero(y_predicted == 1)/len(tweets)*100, \"% positve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets about the Samsung Galaxy Watch are  39.22924901185771 % positve\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "\n",
    "q = 'Samsung Galaxy Watch'\n",
    "count = 100\n",
    "search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "statuses = search_results['statuses']\n",
    "\n",
    "# Iterating through more batches of results by following the cursor\n",
    "while len(tweets) < 1000:\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    except KeyError as e: # No more results when next_results doesn't exist\n",
    "        break\n",
    "\n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=847960489447628799&q=%23RIPSelena&count=100&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
    "\n",
    "    search_results = twitter_api.search.tweets(**kwargs)\n",
    "    statuses = search_results['statuses']\n",
    "    for i in statuses:\n",
    "        if (not ('retweeted_status' in i.keys())) and i['lang'] == 'en':\n",
    "            tweets.append(str(i['text']))\n",
    "    \n",
    "tweets2 = []               \n",
    "for i in tweets:\n",
    "    tweets2.append(bytes(i, 'utf-8'))\n",
    "Xtest = X.transform(tweets2)\n",
    "y_predicted = y_predictor.predict(Xtest)\n",
    "\n",
    "print(\"Tweets about the Samsung Galaxy Watch are \", np.count_nonzero(y_predicted == 1)/len(tweets)*100, \"% positve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets about Chromebooks are  8.649173955296405 % positve\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "\n",
    "q = 'Chromebook'\n",
    "count = 100\n",
    "search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "statuses = search_results['statuses']\n",
    "\n",
    "# Iterating through more batches of results by following the cursor\n",
    "while len(tweets) < 1000:\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    except KeyError as e: # No more results when next_results doesn't exist\n",
    "        break\n",
    "\n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=847960489447628799&q=%23RIPSelena&count=100&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
    "\n",
    "    search_results = twitter_api.search.tweets(**kwargs)\n",
    "    statuses = search_results['statuses']\n",
    "    for i in statuses:\n",
    "        if (not ('retweeted_status' in i.keys())) and i['lang'] == 'en':\n",
    "            tweets.append(str(i['text']))\n",
    "    \n",
    "tweets2 = []               \n",
    "for i in tweets:\n",
    "    tweets2.append(bytes(i, 'utf-8'))\n",
    "Xtest = X.transform(tweets2)\n",
    "y_predicted = y_predictor.predict(Xtest)\n",
    "print(\"Tweets about Chromebooks are \", np.count_nonzero(y_predicted == 1)/len(tweets)*100, \"% positve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets about Samsung are  12.574257425742575 % positve\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "\n",
    "q = 'Samsung'\n",
    "count = 100\n",
    "search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "statuses = search_results['statuses']\n",
    "\n",
    "# Iterating through more batches of results by following the cursor\n",
    "while len(tweets) < 1000:\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    except KeyError as e: # No more results when next_results doesn't exist\n",
    "        break\n",
    "\n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=847960489447628799&q=%23RIPSelena&count=100&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
    "\n",
    "    search_results = twitter_api.search.tweets(**kwargs)\n",
    "    statuses = search_results['statuses']\n",
    "    for i in statuses:\n",
    "        if (not ('retweeted_status' in i.keys())) and i['lang'] == 'en':\n",
    "            tweets.append(str(i['text']))\n",
    "    \n",
    "tweets2 = []               \n",
    "for i in tweets:\n",
    "    tweets2.append(bytes(i, 'utf-8'))\n",
    "Xtest = X.transform(tweets2)\n",
    "y_predicted = y_predictor.predict(Xtest)\n",
    "print(\"Tweets about Samsung are \", np.count_nonzero(y_predicted == 1)/len(tweets)*100, \"% positve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "0.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
